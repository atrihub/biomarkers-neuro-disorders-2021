---
title: Biostatistics for Longitudinal Data
output: 
  xaringan::moon_reader:
    self_contained:  true
    css: ["default", "default-fonts", "./css/styles.css"]
    seal: false 
    lib_dir: libs
    nature:
      # autoplay: 5000
      highlightStyle: solarized-light
      highlightLanguage: ["r", "css", "yaml"]
      # slideNumberFormat: "" 
      highlightLines: true
      countIncrementalSlides: false
      ratio: "16:9"
      titleSlideClass: [top, right]
---

class: middle, center

# Biostatistics for Longitudinal Data

Michael Donohue, PhD

University of Southern California

### Biomarkers in Neurodegenerative Disorders

University of Gothenburg

May 26, 2021

```{r setup, echo=FALSE, message=FALSE, warning=FALSE}
# For ADNIMERGE, go to http://adni.loni.usc.edu/, https://adni.bitbucket.io/

library(Hmisc)
library(knitr)
library(kableExtra)
library(gridExtra)
library(plotly)
library(nlme)
library(emmeans)
library(multcomp)
library(arsenal)
library(grid)
library(gridExtra)
library(mvtnorm)
library(mice)
library(tidyverse)

options(digits=3)

theme_set(theme_bw())

# http://www.cookbook-r.com/Graphs/Colors_(ggplot2)/#a-colorblind-friendly-palette
cbbPalette <-
    c("#0072B2", "#D55E00", "#E69F00",
      "#009E73", "#F0E442", "#999999",
      "#000000", "#56B4E9", "#CC79A7")
scale_colour_discrete <-
    function(...) scale_colour_manual(..., values = cbbPalette)
scale_fill_discrete <-
    function(...) scale_fill_manual(..., values = cbbPalette)
scale_colour_discrete <-
    function(...) scale_colour_manual(..., values = cbbPalette)
scale_fill_discrete <-
    function(...) scale_fill_manual(..., values = cbbPalette)

theme_table <- function(..., levs=2){
  theme_minimal(...) + 
    theme(
      panel.grid = element_blank(), 
      axis.text.x = element_blank(),
      axis.text.y = element_text(face='bold', color=cbbPalette[1:levs]),
      axis.title = element_blank())
}

load('simulated-trial.Rdata')
trial_obs <- trial_obs %>%
  arrange(id, month)
```
  
```{r knitr-options, echo=FALSE, message=FALSE, warning=FALSE, purl=FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = NA,
  echo = FALSE, cache = TRUE, 
  cache.path = 'long_cache/',
  fig.path = 'long_fig/',
  dev='svg',
  tidy=FALSE,
  out.extra = '',
  out.width='100%',
  fig.align = 'center', crop = TRUE, fig.pos = '!h', 
  fig.height=3, fig.width=3*2.2,
  message = FALSE, 
  warning = FALSE
)
# knitr::knit_hooks$set(inline = function(x) {
#   prettyNum(x, big.mark=",")
# })
# purl('longitudinal.Rmd')
```


```{r load_refs, include=FALSE, cache=FALSE, purl=FALSE}
library(RefManageR)
BibOptions(check.entries = FALSE,
  bib.style = "authoryear",
  cite.style = "authoryear",
  style = "markdown",
  hyperlink = FALSE,
  dashed = FALSE,
  max.names = 1)
bib <- ReadBib("./references.bib", check = FALSE)
NoCite(bib, c('fitzmaurice2012applied', 'molenberghs2007missing', 'verbeke2000linear', 'diggle2002analysis', 'mallinckrodt2003assessing', 'pinheiro2006mixed', 'donohue2012mixed', 'little2019statistical', 'buuren2010mice'))
# Citet(), Citep(), AutoCite()
```

.pull-left[

```{r echo=FALSE, fig.align='center', out.width='57%', purl=FALSE}
knitr::include_graphics("./images/atri.png")
```

]

.pull-right[

```{r echo=FALSE, fig.align='center', out.width='47%', purl=FALSE}
knitr::include_graphics("./images/actc_logo.png")
```

]

---

# Session 3 Outline

.large[
- Mixed effect models
- Mean & Variance Structure
- Mixed Model for Repeated Measures (MMRM)
- Constrained Longitudinal Analysis (cLDA)
- Model selection strategies
- Nested random effects
- Missing Data, MAR, MNAR
- Multiple Imputation & Delta Method
]

---

class: inverse, middle, center

# Mixed-effects Models

---

# Linear mixed-effects model (LME)

Linear mixed-effects models provide a cleaner, more efficient, and more accurate one-step alternative to two-stage models

$$\left. \begin{array}{ll}
\textrm{Stage 1: } \textrm{ADAS}_{ij} &= \beta_{0i} + t_{ij}\beta_{1i} + \varepsilon_{ij}\\
\textrm{Stage 2: } \hat\beta_{1i} &= X_i\beta + \varepsilon'_{ij}
  \end{array} \right\} \rightarrow
  \textrm{ADAS}_{ij} = X_i\beta + b_{0i} + t_{ij}b_{1i} + \varepsilon_{ij}$$

* $X_i$: covariates for subject $i=1,\ldots,400$
* $\beta$: population level _fixed effects_
* $b_i \sim \mathcal{N}(0,D)$: subject-specific _random effects_ for subject $i=1,\ldots,400$
* $(\varepsilon_{i1},\ldots,\varepsilon_{i4}) \sim \mathcal{N}(0,\Sigma)$: vector of _residuals_ for subject $i=1,\ldots,400$
* $D,\, \Sigma$: _variance components_

$b_1,\ldots,b_N,\varepsilon_1,\ldots,\varepsilon_N$ are assumed independent

---

# Linear mixed-effects models of simulated trial

```{r trial_lme, size = 'tiny'}
fit_lme <- lme(ADAS11 ~ month + month:active, data = trial_obs, random = ~month|id)
summary(fit_lme)
```

---

# LME model with additional covariates

```{r trial_lme_age, size = 'tiny'}
fit_lme_cov <- lme(ADAS11 ~ age_c + female + month + month:active, data = trial_obs, random = ~month|id)
summary(fit_lme_cov)
```

---

# Linear mixed-effects models (R code)

```{r trial_lme_rcode, eval = FALSE, echo = TRUE}
lme(ADAS11 ~ month + month:active, 
  data = trial_obs, random = ~month|id)

lme(ADAS11 ~ age_c + female + month + month:active, 
  data = trial_obs, random = ~month|id)
```

```{r trial_lme_profiles, echo = FALSE, size = 'scriptsize'}
em <- fit_lme_cov %>%
  ref_grid(at = list(
    month = unique(trial_obs$month),
    active = unique(trial_obs$active),
    female = 1, age.c = 0)) %>%
  emmeans(specs = c('active', 'month')) %>%
  as.data.frame()
```

---

# Mean profiles

```{r echo = FALSE, size = 'scriptsize'}
em %>% kable()
```

---

# Modeled mean profiles (shaded CIs)

```{r echo = FALSE, size = 'scriptsize'}
p <- em %>%
  mutate(group = factor(active, levels = c(0,1), labels = c('Placebo', 'Active'))) %>%
  ggplot(aes(x = month, y = emmean, group = group)) +
  geom_line(aes(color=group)) +
  geom_ribbon(aes(ymin = lower.CL, ymax = upper.CL, fill=group), alpha=0.25) +
  scale_x_continuous(breaks=months) +
  ylab('Mean ADAS (95% CI)') +
  theme(legend.position=c(0.1, 0.75))
grid.draw(arrangeGrob(p,countTab,heights=c(3,1)))
```

---

# Plotting profiles (error bar CIs)

```{r echo = FALSE, size = 'scriptsize'}
p <- em %>%
  mutate(group = factor(active, levels = c(0,1), labels = c('Placebo', 'Active'))) %>%
  ggplot(aes(x = month, y = emmean, group = group, color=group))+
  geom_line() +
  geom_errorbar(aes(ymin = lower.CL, ymax = upper.CL), width=0, position=position_dodge(0.2)) +
  ylab('Mean ADAS (95% CI)') +
  scale_x_continuous(breaks=months) +
  ylab('Mean ADAS (95% CI)') +
  theme(legend.position=c(0.1, 0.75))
grid.draw(arrangeGrob(p,countTab,heights=c(3,1)))
```


```{r varPar, echo=FALSE}
# simulate data with different variance parameters
varPar <- expand.grid(
  sigma_random_intercept = c(2, 10),
  sigma_random_slope = c(0.2, 0.75),
  sigma_residual = c(2, 8)
)

varPlot <- do.call(rbind, lapply(1:nrow(varPar), function(i){
  set.seed(20170714)
  subjects <- data.frame(
    id = 1:(2*n),
    active = sample(c(rep(0,n), rep(1,n)), 2*n),
    female = sample(0:1, 2*n, replace=TRUE),
    age = rnorm(2*n, 75, 7.8),
    censor = rexp(2*n,rate=attrition_rate),
    sigma_random_intercept = varPar[i, 'sigma_random_intercept'],
    sigma_random_slope = varPar[i, 'sigma_random_slope'],
    sigma_residual = varPar[i, 'sigma_residual'],
    ran.intercept = rnorm(2*n, sd=varPar[i, 'sigma_random_intercept']),
    ran.slope     = rnorm(2*n, sd=varPar[i, 'sigma_random_slope'])) %>%
    mutate(age_c = age - mean(age))

  trial <- right_join(subjects, 
    expand.grid(id = 1:(2*n), month=months)) %>%
    mutate(
      residual = rnorm(2*n*length(months), sd=varPar[i, 'sigma_residual']),
      group = factor(active, 0:1, c('placebo', 'active')),
      missing = ifelse(month>censor, 1, 0)) %>%
    arrange(id, month) %>%
    filter(!missing)
  trial$ADAS11 <- round(
    model.matrix(~ female+age_c+month+month:active, data = trial)[, names(Beta)] %*% 
    Beta +
    with(trial, ran.intercept + ran.slope*month + residual), 
    digits = 0
  )[,1]
  trial
}))
```

---

# Mixed effect models: standard deviation of residuals

```{r }
ggplot(filter(varPlot, sigma_random_intercept==2 & sigma_random_slope==0.2), 
  aes(x=month, y=ADAS11, group=id, color=group)) + 
  geom_line(alpha=0.25) +
  geom_smooth(aes(group = NULL), method = 'lm', size = 2) +
  facet_wrap(~paste('Residual SD =', sigma_residual)) +
  ylim(0,70) +
  scale_x_continuous(breaks=months) +
  theme(legend.position=c(0.1,0.8))
```

---

# Mixed effect models: standard deviation of random intercepts

```{r }
ggplot(filter(varPlot, sigma_residual==2 & sigma_random_slope==0.2), 
  aes(x=month, y=ADAS11, group=id, color=group)) + 
  geom_line(alpha=0.25) +
  geom_smooth(aes(group = NULL), method = 'lm', size = 2) +
  facet_wrap(~paste('Random intercept SD =', sigma_random_intercept)) +
  ylim(0,70) +
  scale_x_continuous(breaks=months) +
  theme(legend.position=c(0.6,0.8))
```

---

# Mixed effect models: standard deviation of random slopes

```{r }
ggplot(filter(varPlot, sigma_residual==2 & sigma_random_intercept==2), 
  aes(x=month, y=ADAS11, group=id, color=group)) + 
  geom_line(alpha=0.25) +
  geom_smooth(aes(group = NULL), method = 'lm', size = 2) +
  facet_wrap(~paste('Random slope SD =', sigma_random_slope)) +
  ylim(0,70) +
  scale_x_continuous(breaks=months) +
  theme(legend.position=c(0.1,0.8))
```

---

# Random intercepts model

* NOTE: With only two timepoints, it is impossible to fit a model with random slopes
* If we drop the _random slope_ term, $t_{ij}b_{1i}$, what remains is called a _random intercepts_ model:

$$\textrm{ADAS}_{ij} = X_i\beta + b_{0i} + \varepsilon_i$$
---

# Random intercepts model

.large[
```{r trial_lme_apoe_int, size = 'tiny'}
fit_lme_int <- update(fit_lme, random = ~1|id)
summary(fit_lme_int)
```
]

---

# Random intercepts model vs model with random slopes

```{r trial_lme_apoe_int_vs_slope, size = 'footnotesize', echo = TRUE}
anova(fit_lme_int, fit_lme)
```

The model with random slopes is preferred (smaller AIC is better)

---

class: inverse, middle, center

# Mean & Variance Structure

---

# Key features of longitudinal models

.large[

* Last session we discussed a linear mixed effect model (LME) which treats time as a continuous variable
* This is one type of longitudinal model among many types
* Three key features of longitudinal models:
  * **Mean structure**: Governs shape of mean trajectory
  * **Variance-covariance structure**: Governs within-subject correlation
  * **Baseline assessment**: Treated as covariate or outcome

]

---

# Taxonomy of common longitudinal models

|                        | **MMRM**             | **LDA**              | **cLDA**             |
|------------------------|----------------------|----------------------|----------------------|
|**Mean structure**      | cat. time            | cat. or cts. time    | cat. or cts. time    |
|**Variance-covariance** | correlated residuals | correlated residuals | correlated residuals |
|                        |                      | or random effects    |  or random effects   |
|**Baseline assessment** | as covariate         | as outcome;          | as outcome;          |
|                        |                      | different means      |  common mean         | 
|                        |                      | per group            |  per group           |
|**Appropriate use**     | Randomized groups    | Non-randomized groups| Randomized groups    |

Abbreviations: 

* MMRM: Mixed Model of Repeated Measures
* LDA: Longitudinal Data Analysis
* cLDA: Constrained LDA
* cat.: categorical
* cts.: continuous

---

# Linear mixed-effects model (LME)

$$\textrm{ADAS}_{ij} = X_i\beta + b_{0i} + t_{ij}b_{1i} + \varepsilon_{ij}$$
* $X_i$: covariates for subject $i=1,\ldots,400$
* $\beta$: population level _fixed effects_
* $b_i \sim \mathcal{N}(0,D)$: subject-specific _random effects_ for subject $i=1,\ldots,400$
* $(\varepsilon_{i1},\ldots,\varepsilon_{i4}) \sim \mathcal{N}(0,\Sigma)$: vector of _residuals_ for subject $i=1,\ldots,400$
* $D,\, \Sigma$: _variance components_


$b_1,\ldots,b_N,\varepsilon_1,\ldots,\varepsilon_N$ are assumed independent

---

# Linear mixed-effects model (LME)

$$\textrm{ADAS}_{ij} = X_i\beta + b_{0i} + t_{ij}b_{1i} + \varepsilon_{ij}$$

* _Mean structure_ (fixed effects): $X_i\beta$
    * linear, quadratic, splines, or categorical time; and
    * other fixed-effect covariates (age, education, gender, treatment)
* _Variance structure_ (random effects & residuals): $b_{0i} + t_{ij}b_{1i} + \varepsilon_{ij}$
    * random intercept, random intercept & slope; or
    * impose variance-covariance (variance and correlation) structure on residuals $(\varepsilon_{i1},\ldots,\varepsilon_{i4}) \sim \mathcal{N}(0,\Sigma)$

---

class: inverse, middle, center

# Mixed Model of Repeated Measures (MMRM)

---

# Mixed Model of Repeated Measures (MMRM)

.large[
* A popular _marginal model_, "Mixed Model of Repeated Measures" (MMRM), makes no (or very general) assumptions about the mean and variance/covariance structure.
* **Unstructured mean:** time is treated as categorical, so no linear (or quadratic, etc.) trend is assumed
* **Unstructured variance/covariance:** includes parameters for variance at each visit ("heterogeneous") and visit-to-visit correlation
* A repeated measures extension of ANCOVA (instead of one post-baseline assessment, there are several)
* MMRM is not actually a mixed-effects model (no random effects)!
* Usually treats change from baseline as outcome variable, and baseline as covariate (like ANCOVA).
* Requires categorizing/binning time (this has become problematic for clinical trials interrupted by COVID, for example) 
]

---

# Mean structure examples

```{r }
means <- expand.grid(female=1, age_c=0, month=0:18, active=0, id=1:3) %>%
  filter(id %in% c(1,2) | month %in% months) 
means <- mutate(means,
    ADAS11 = (model.matrix(~ female+age_c+month+month:active, means)[, names(Beta)] %*% Beta)[,1],
    ADAS11 = replace(ADAS11, ADAS11<0, 0),
    ADAS11 = replace(ADAS11, ADAS11>70, 70),
    ADAS11 = replace(ADAS11, id == 2, ADAS11 + month*month*0.1),
    ADAS11 = replace(ADAS11, id == 3 & month == 0, ADAS11 + 0),
    ADAS11 = replace(ADAS11, id == 3 & month == 6, ADAS11 + 10),
    ADAS11 = replace(ADAS11, id == 3 & month == 12, ADAS11 + 2),
    ADAS11 = replace(ADAS11, id == 3 & month == 18, ADAS11 + 25)) %>%
  filter(id %in% 1:3) %>%
  mutate(Mean = factor(id, labels=c('linear', 'quadratic', 'categorical')))

ggplot(means, aes(x=month, y=ADAS11, group=Mean, color=Mean)) + 
  geom_line() +
  scale_x_continuous(breaks=months) +
  theme(legend.position=c(0.15,0.7))
```

---

# Variance-covariance structure

$$\textrm{ADAS}_{ij} = X_i\beta + b_{0i} + t_{ij}b_{1i} + \varepsilon_{ij}$$
* _Vanilla_ LME assumes _homoscedastic_ (homogenous/constant variance), independent residuals
* MMRM drops the $b$ terms (random effects) and can assume _heteroscedastic_ (heterogeneous variance), correlated residuals
  * $(\varepsilon_{i1},\ldots,\varepsilon_{i4}) \sim \mathcal{N}(0,\Sigma)$
  * $\Sigma = \sigma^2VCV$, where
  * $\sigma^2$ is variance scale parameter
  * $V$ is diagonal matrix of standard deviation weights
  * $C$ is correlation matrix

---

# Variance-covariance structure


$$(\varepsilon_{i1},\ldots,\varepsilon_{i4}) \sim \mathcal{N}(0,\Sigma),\quad \Sigma = \sigma^2VCV$$

Examples:

```console
                        Heterogeneous variance
                                0  6 12 18
                             0  1  0  0  0
                        V =  6  0  2  0  0
                            12  0  0  7  0
                            18  0  0  0 10
```

```console
 Exchangeable/Compound Symmetric              Unstructured/Symmetric
             0    6   12   18                       0    6   12   18
        0  1.0  0.2  0.2  0.2                  0  1.0  0.3  0.2  0.1
    C = 6  0.2  1.0  0.2  0.2             C =  6  0.3  1.0  0.4  0.2
       12  0.2  0.2  1.0  0.2                 12  0.2  0.4  1.0  0.2
       18  0.2  0.2  0.2  1.0                 12  0.1  0.2  0.2  1.0
```

```{r varPar2, echo=FALSE}
# simulate data with different variance parameters
varPar <- expand.grid(
  variance = c('homogeneous', 'heterogeneous'),
  correlation = c('uncorrelated', 'correlated')
)

SD <- list(homogeneous = sqrt(rep(4, 4)), heterogeneous = (1:4)*2)
Cor <- list(uncorrelated = 0, correlated = 0.8)

varPlot <- do.call(rbind, lapply(1:nrow(varPar), function(i){
  set.seed(20170714)
  subjects <- data.frame(
    id = 1:(2*n),
    active = sample(c(rep(0,n), rep(1,n)), 2*n),
    female = sample(0:1, 2*n, replace=TRUE),
    age_c = rnorm(2*n, 0, 7.8),
    censor = rexp(2*n,rate=attrition_rate))
    
  vv <- diag(SD[[varPar[i,'variance']]])
  cc <- matrix(Cor[[varPar[i,'correlation']]], nrow=4, ncol=4)
  diag(cc) <- 1
  resids <- as.numeric(t(rmvnorm(nrow(subjects), mean=rep(0,4), sigma=vv%*%cc%*%vv)))

  trial <- right_join(subjects, 
    expand.grid(id = 1:(2*n), month=months)) %>%
    arrange(id, month) %>%
    mutate(residual = resids,
      group = factor(active, 0:1, c('placebo', 'active')),
      missing = ifelse(month>censor, 1, 0),
      variance = varPar[i,'variance'],
      correlation = varPar[i,'correlation']) %>%
    arrange(id, month) %>%
    filter(!missing)
  trial$ADAS11 <- round(
    model.matrix(~ female+age_c+month+month:active, data = trial)[, names(Beta)] %*% 
    Beta + trial$residual, 
    digits = 0
  )[,1]
  trial
}))
```

---

# Homogeneous vs heterogeneous variance structure

```{r }
ggplot(filter(varPlot, correlation=='correlated'), 
  aes(x=month, y=ADAS11, group=id, color=group)) + 
  geom_line(alpha=0.25) +
  geom_smooth(aes(group = NULL), method = 'lm', size = 2) +
  facet_wrap(~variance) +
  ylim(0,50) +
  scale_x_continuous(breaks=months) +
  theme(legend.position=c(0.1,0.8))
```

???

Right-hand side is more typical of our cognitive outcomes

You can imagine what can go wrong if we assume the left-hand side instead. We would likely underestimated variance for later timepoints.

---

# Uncorrelated vs correlated residuals

```{r }
ggplot(filter(varPlot, variance=='heterogeneous'), 
  aes(x=month, y=ADAS11, group=id, color=group)) + 
  geom_line(alpha=0.25) +
  geom_smooth(aes(group = NULL), method = 'lm', size = 2) +
  facet_wrap(~correlation) +
  ylim(0,50) +
  scale_x_continuous(breaks=months) +
  theme(legend.position='none')
```


```{r echo=FALSE, eval=FALSE}
####################################################
### pilot estimates are from a model fit to ADNI ###
####################################################

# library(ADNIMERGE) # available for loni.usc.edu
# adni_ad <- filter(adnimerge, M<=24 & M!=18 & !is.na(ADAS11) & DX.bl=='AD') %>%
#   mutate(m = as.factor(M),
#      visNo = as.numeric(m))
#
# with(adni_ad, table(m, visNo))
# fit_adni <- gls(ADAS11 ~ PTGENDER + center(AGE) + m, data=adni_ad,
#   correlation = corSymm(form = ~ visNo | RID),
#   weights = varIdent(form = ~ 1 | m) )
# summary(fit_adni)
```


---

# Simulating categorical time data $\ldots$

These are all estimates required:

```{r echo=TRUE}
Beta <- c(
   '(Intercept)'= 19.8, # mean ADAS at baseline
        'female'=-0.51, # female perform better
         'age_c'= 0.04, # worse change for older at baseline (age mean centered)
            'm6'= 2.23, # worsening at month 6 in pbo
           'm12'= 4.46, # worsening at month 12 in pbo
           'm18'= 7.31, # worsening at month 18 in pbo
     'm6:active'=-0.20, # relative improvement at month 6 with treatment
    'm12:active'=-0.70, # relative improvement at month 12 with treatment
    'm18:active'=-1.75) # relative improvement at month 18 with treatment

# other design parameters
months <- c(0, 6, 12, 18)
n <- 200 # per group
attrition_rate <- 0.40/18 # approx per month

# var-cov parameters
SD <- 6.77                          # standard deviation scale parameter
vv <- diag(c(1, 1.2, 1.5, 1.8))          # heterogeneous variance weight matrix
cc <- matrix(0.75, nrow=4, ncol=4)   # correlation matrix
diag(cc) <- 1
```

```{r }
# set seed so that simulation is reproducible
set.seed(20170714)

# simulate subject specific data
subjects <- data.frame(
  id = 1:(2*n),
  active = sample(c(rep(0,n), rep(1,n)), 2*n),
  female = sample(0:1, 2*n, replace=TRUE),
  age = rnorm(2*n, 75, 7.8), 
  censor = rexp(2*n,rate=attrition_rate)) %>%
  mutate(age_c = age-mean(age))
  
# simulate vector of correlated residuals
resids <- as.numeric(t(rmvnorm(nrow(subjects), mean=rep(0,nrow(vv)), sigma=SD^2*vv%*%cc%*%vv)))

# simulate data over time
trial <- right_join(subjects,
  expand.grid(id = 1:(2*n), month=months)) %>%
  arrange(id, month) %>%    ## WARNING: data must be properly sorted by subject and time 
  mutate(residual = resids, ## prior to appending residuals
    group = factor(active, 0:1, c('placebo', 'active')),
    missing = ifelse(month>censor, 1, 0),
    m = as.factor(month),
    visNo = as.numeric(m)) %>%
  arrange(id, month)

# create visit indicators
trial <- cbind(trial, model.matrix(~ -1+m, data = trial))

# calculate the ADAS scores with random effects and residuals and 
# round to nearest digit in 0-70
trial <- mutate(trial,
  ADAS11 = (model.matrix(~ female+age_c+m6+m12+m18+(m6+m12+m18):active, data = trial)[, names(Beta)] %*% Beta)[,1],
  ADAS11 = round(ADAS11 + residual, digits = 0),
  ADAS11 = replace(ADAS11, ADAS11<0, 0),
  ADAS11 = replace(ADAS11, ADAS11>70, 70))

# filter out the missing observations
trial_obs <- filter(trial, !missing)

# transfrom data from long to wide
trial_wide <- trial_obs %>%
  select(id, month, female, age, age_c, active, group, ADAS11) %>% 
  mutate(month = paste0('ADAS11.m', month)) %>%
  spread(month, ADAS11) %>%
  select(id:group, ADAS11.m0, ADAS11.m6, ADAS11.m12, ADAS11.m18)

# data for MMRM
trial_mmrm <- right_join(
  select(trial_wide, id, ADAS11.m0), 
  filter(trial_obs, month>0)) %>%
  mutate(ADAS11.ch = ADAS11 - ADAS11.m0,
    m = as.factor(month),
    visNo = as.numeric(m))
```

---

# The pseudo categorical time data

```{r }
trial_mmrm %>% select(-group, -missing, -month, -visNo) %>% 
  select(id, m, m0, m6, m12, m18, everything()) %>%
  head() %>%
  kable()
```

---

# The pseudo categorical time data

```{r spaghetti_plot}
ggplot(trial, aes(x=month, y=ADAS11, group=id, color=group)) + 
  geom_line(alpha=0.25) +
  geom_smooth(aes(group = NULL), method = 'loess', size = 2) +
  scale_x_continuous(breaks=months, lim=c(0,18)) +
  theme(legend.position=c(0.1, 0.85), legend.background = element_rect(fill=NA))
```

---

# Fitting MMRM in R

```{r echo = TRUE}
# Symmetric correlation, heterogeneous variance
MMRMsymHet <- gls(ADAS11.ch ~ 
  -1+ADAS11.m0+female+age_c+(m6+m12+m18)+(m6+m12+m18):active,
  data=trial_mmrm, correlation = corSymm(form = ~ visNo | id),
  weights = varIdent(form = ~ 1 | m) )

# Compound Symmetric correlation, heterogeneous variance
MMRMcompSymHet <- gls(ADAS11.ch ~ 
  -1+ADAS11.m0+female+age_c+(m6+m12+m18)+(m6+m12+m18):active,
  data=trial_mmrm, correlation = corCompSymm(form = ~ visNo | id),
  weights = varIdent(form = ~ 1 | m) )
```

See `?corClasses` and `?varClasses` in `R`; and chapter 5 in `r Citet(bib, 'pinheiro2006mixed')`
for more details.

---

# MMRM summaries in R

```{r echo=TRUE, eval=FALSE}
summary(MMRMsymHet)
```

```{r }
x <- summary(MMRMsymHet)
print(summary(x$modelStruct), sigma = x$sigma)
```

---

# MMRM summaries in R

```{r }
cat('Coefficients:\n')
xtTab <- as.data.frame(x$tTable)
printCoefmat(x$tTable, eps=0.001, digits=3)
# cat("\nStandardized residuals:\n")
# print(x$residuals)
cat("\n")
cat("Residual standard error:", format(x$sigma),"\n")
# cat("Degrees of freedom:", dd[["N"]],"total;",dd[["N"]] - dd[["p"]],
#     "residual\n")
```

---

class: inverse, middle, center

# Constrained Longitudinal Data Analysis (cLDA)

---

# Fitting cLDA in R

```{r echo = TRUE}
# Symmetric correlation, heterogeneous variance
cLDAsymHet <- gls(ADAS11 ~ 
  -1+female+age_c+m0+(m6+m12+m18)+(m6+m12+m18):active,
  data=trial_obs, correlation = corSymm(form = ~ visNo | id),
  weights = varIdent(form = ~ 1 | m) )

# Compound Symmetric correlation, heterogeneous variance
cLDAcompSymHet <- gls(ADAS11 ~ 
  -1+female+age_c+m0+(m6+m12+m18)+(m6+m12+m18):active,
  data=trial_obs, correlation = corCompSymm(form = ~ visNo | id),
  weights = varIdent(form = ~ 1 | m) )
```

---

# cLDA summaries in R
```{r }
x <- summary(cLDAsymHet)
print(summary(x$modelStruct), sigma = x$sigma)
```

---

# MMRM summaries in R

```{r }
cat('Coefficients:\n')
xtTab <- as.data.frame(x$tTable)
printCoefmat(x$tTable, eps=0.001, digits=3)
# cat("\nStandardized residuals:\n")
# print(x$residuals)
cat("\n")
cat("Residual standard error:", format(x$sigma),"\n")
# cat("Degrees of freedom:", dd[["N"]],"total;",dd[["N"]] - dd[["p"]],
#     "residual\n")
```

---

# Fitting continuous time cLDA in R

```{r echo = TRUE}
# Linear time
cLDAlin <- gls(ADAS11 ~ 
  female + age_c + month + month:active,
  data=trial_obs, correlation = corSymm(form = ~ visNo | id),
  weights = varIdent(form = ~ 1 | m))

# Quadratic time
cLDAquad <- gls(ADAS11 ~ 
  female + age_c + (month + I(month^2)) + (month + I(month^2)):active,
  data=trial_obs, correlation = corSymm(form = ~ visNo | id),
  weights = varIdent(form = ~ 1 | m))
```

```{r echo = FALSE}
# Natural cubic spline
b1 <- function(t){
  as.numeric(predict(splines::ns(trial_obs$month, df=2), t)[,1])
}
b2 <- function(t){
  as.numeric(predict(splines::ns(trial_obs$month, df=2), t)[,2])
}
cLDAncs <- gls(ADAS11 ~ 
  female + age_c + (b1(month) + b2(month)) + (b1(month) + b2(month)):active,
  data=trial_obs, correlation = corSymm(form = ~ visNo | id),
  weights = varIdent(form = ~ 1 | m))
```

```{r echo = FALSE, size = 'scriptsize'}
plotData0 <- filter(trial_obs, !duplicated(paste(month, active))) %>%
  arrange(active, month) %>%
  mutate(female = 1, age_c=0) %>%
  dplyr::select(-age, -censor, -residual, -ADAS11)

plotMatrix_cat <- model.matrix(~ -1 + female + age_c + m0 + (m6 + m12 + m18) + 
  (m6 + m12 + m18):active, 
  data = plotData0)

plotMatrix_quad <- model.matrix(~ female + age_c + (month + I(month^2)) + 
  (month + I(month^2)):active, 
  data = plotData0)

plotMatrix_ncs <- model.matrix(~ female + age_c + (b1(month) + b2(month)) + 
  (b1(month) + b2(month)):active,
  data = plotData0)

plotData <- bind_rows(
  bind_cols(plotData0, confint(glht(cLDAsymHet, linfct = plotMatrix_cat))$confint %>%
      as.data.frame()) %>%
    mutate(model = 'categorical time'),
  bind_cols(plotData0, confint(glht(cLDAquad, linfct = plotMatrix_quad))$confint %>%
      as.data.frame()) %>%
    mutate(model = 'quadratic time'),
  bind_cols(plotData0, confint(glht(cLDAncs, linfct = plotMatrix_ncs))$confint %>%
      as.data.frame()) %>%
    mutate(model = 'natural cubic splines'))
```

---

# Modeled mean profiles

```{r }
summaryTable <- trial_obs %>% 
  group_by(group, month) %>%
  summarise(
    n=length(ADAS11),
    mean=mean(ADAS11),
    sd=sd(ADAS11),
    lower95 = smean.cl.normal(ADAS11)[['Lower']],
    upper95 = smean.cl.normal(ADAS11)[['Upper']],
    min=min(ADAS11),
    max=max(ADAS11))
countTab <- ggplot(summaryTable, aes(x=month, y=group, label=n)) + geom_text() + theme_table()

ggplot(plotData, aes(x = month, y = Estimate))+
  geom_line(aes(color=group, linetype=model), position=position_dodge(width=2)) +
  geom_point(aes(color=group, shape=model), position=position_dodge(width=2)) +
  ylim(c(15,30)) +
  scale_x_continuous(breaks=months) +
  ylab('Mean ADAS (95% CI)') +
  theme(legend.position='top')
```

```{r }
contrastData0 <- filter(trial_obs, !duplicated(paste(month, active))) %>%
  arrange(active, month) %>%
  filter(active==1 & month>0) %>%
  mutate(female = 1, age_c=0) %>%
  dplyr::select(-age, -censor, -residual, -ADAS11)

contrastMatrix_cat <- as.data.frame(model.matrix(~ -1 + female + age_c + m0 + (m6 + m12 + m18) + (m6 + m12 + m18):active, 
  data = contrastData0)) %>%
  mutate(female=0, m0=0, m6=0, m12=0, m18=0)

contrastMatrix_quad <- as.data.frame(model.matrix(~ female + age_c + (month + I(month^2)) + (month + I(month^2)):active, 
  data = contrastData0)) %>%
  mutate('(Intercept)' = 0, female=0, month=0, 'I(month^2)' = 0)

contrastMatrix_ncs <- as.data.frame(model.matrix(~ female + age_c + (b1(month) + b2(month)) + (b1(month) + b2(month)):active, 
  data = contrastData0)) %>%
  mutate('(Intercept)' = 0, female=0, 'b1(month)'=0, 'b2(month)' = 0)

rownames(contrastMatrix_quad) <- rownames(contrastMatrix_cat) <- 
  rownames(contrastMatrix_ncs) <-paste0('m', months[-1])
```

---

# cLDA with Natural Cubic Splines

.pull-leftWider[

```{r }
plotData %>%
  filter(model == 'natural cubic splines') %>%
  ggplot(aes(x = month, y = Estimate))+
  geom_line(aes(color=group)) +
  geom_point(aes(color=group)) +
  ylim(c(15,30)) +
  scale_x_continuous(breaks=months) +
  ylab('Mean ADAS (95% CI)') +
  theme(legend.position='top')
```

]

.pull-rightNarrower[


Natural Cubic Spline `r Citep(bib=bib, 'schoenberg1964spline')`:

+ Group 1 mean at $t$: $f_1(t)$

+ Group 2 mean at $t$: $f_2(t)$

+ $f_1$ and $f_2$ are two smooth functions

  + cubic basis functions

  + "natural" implies second derivatives are zero at boundaries

]

???

This model uses natural cubic splines to model the mean in each group. In this case we assume:

- cubic spline basis functions,  
- just one interior knot, and
- second derivatives are fix at 0 on the boundaries.

This constraint helps to avoids spurious flips of the curves near the last observations.

---

# Natural Cubic Splines (df=2)

Assumes the smooth functions for the mean in each group can each be expressed:

$$ f(t) = b_1(t)\beta_1 + b_2(t)\beta_2 $$
where 

- $b$s are *known functions* (cubic polynomials) that depend only on "knot" locations (t=0, median observation time, and last observation time). 

- $\beta$s are unknown fixed effect regression coefficients estimated as usual by `gls` or `PROC MIXED`

- resulting $f$ has second derivative of 0 at t=0 and last observation time.

Fit in `R`, using `nlme::gls`:

```{r eval=FALSE, echo=TRUE}
gls(ADAS11 ~  female + age_c +   # bl covs
    ns(months, df=2) +        # natural spline for placebo (1 knot)
    ns(months, df=2):active + # natural spline for active (1 knot)
  correlation = corSymm(form = ~ visNo | id), # general correlation
  weights = varIdent(form = ~ 1 | visNo))     # heterogeneous variance
```

???

And once we have the basis expansion, this is also just a linear model which can 
be fit with gls.

One downside is that we have to pre-specify the number and location of spline knots.

Inference could be based on a likelihood ratio for the main group effect, or we could
derive contrasts at any given timepoint.

We are also exploring using time-varying test-version effects, which seems to be an
efficient way to model the zig-zag mean trajectory we often see due to varying test
difficulty.

---

## Modeled group contrasts for categorical, quadratic, and natrual cubic spline models

```{r }
cts <- bind_rows(
  (glht(cLDAsymHet, linfct = as.matrix(contrastMatrix_cat)) %>% confint())$confint %>%
    as.data.frame() %>%
    mutate(model = 'categorical time'),
  (glht(cLDAquad, linfct = as.matrix(contrastMatrix_quad)) %>% confint())$confint %>%
    as.data.frame() %>%
    mutate(model = 'quadratic time'),
  (glht(cLDAncs, linfct = as.matrix(contrastMatrix_ncs)) %>% confint())$confint %>%
    as.data.frame() %>%
    mutate(model = 'natural cubic splines')) %>%
  mutate(month = rep(months[-1], 3))

ggplot(cts, aes(x=month, y=Estimate, color=model)) +
  geom_point(aes(shape=model), position=position_dodge(width=2)) +
  geom_errorbar(aes(x=month, ymax=upr, ymin=lwr), position=position_dodge(width=2))
```

---

## Modeled group contrasts for quadratic and categorical time models

```{r }
summary(glht(cLDAsymHet, linfct = as.matrix(contrastMatrix_cat)))
```

---

# Taxonomy of common longitudinal models

|                        | **MMRM**             | **LDA**              | **cLDA**             |
|------------------------|----------------------|----------------------|----------------------|
|**Mean structure**      | cat. time            | cat. or cts. time    | cat. or cts. time    |
|**Variance-covariance** | correlated residuals | correlated residuals | correlated residuals |
|                        |                      | or random effects    |  or random effects   |
|**Baseline assessment** | as covariate         | as outcome;          | as outcome;          |
|                        |                      | different means      |  common mean         | 
|                        |                      | per group            |  per group           |
|**Appropriate use**     | Randomized groups    | Non-randomized groups| Randomized groups    |

Abbreviations: 

* MMRM: Mixed Model of Repeated Measures
* LDA: Longitudinal Data Analysis
* cLDA: Constrained LDA
* cat.: categorical
* cts.: continuous

---

# MMRM vs cLDA

* Both (can use) categorical time
* **MMRM** has _change from baseline_ as outcome, baseline assessment as covariate
  * need follow-up data to calculate change
  * modified Intention-to-Treat (mITT) population (i.e. randomized and at least one follow-up assessment)
* **cLDA** uses raw assessment scores as outcome; baseline assessment as outcome, constrains two groups to have same mean
  * only need one observation to enter into analysis
  * _unmodified_ Intention-to-Treat population (i.e. randomized with at least one assessment)
  * baseline mean constraint really only appropriate in randomized studies
* In our simulated trial it is a difference of $n=46$ subjects

Abbreviations: MMRM, Mixed Model of Repeated Measures; LDA, Longitudinal Data Analysis; cLDA, Constrained LDA.

---

# MMRM vs cLDA

From `r Citet(bib, 'lu2010efficiency')`:

* (categorical time) cLDA more powerful than MMRM (aka "longitudinal ANCOVA") when baseline assessments are missing
* cLDA advantage is greater when correlation between baseline and follow-up is weaker

Caveat: Implicitly assumes data Missing at Random.

In simulations of Preclinical Alzheimer's clinical trials, we have found power can increase from 80% with MMRM to about 90% with continuous-time cLDA.

---

# Natural Cubic Splines with **test version effect** in ADNI and HABS

```{r echo=FALSE, fig.align='center', out.width='100%', purl=FALSE}
knitr::include_graphics("./images/modeled-means-1.png")
```

???

Here are models fit to ADNI (top) and HABS (bottom)

The left panel is MMRM with categorical time

The middle and right panels are from the same natural cubic spline model, but one shows the estimated test version effect and the other hides it.

The test version letters are shown along the bottom of each panel.

HABS had more consistent versioning than ADNI.

You can see that the first time version A re-appears at year 3, both amyloid positive and negative groups bump upward with higher scores.

The spline model with version effect captures this quite nicely and in both cases we get a smooth representation of the trend over time removing the nuisance version effects.

---

class: inverse, middle, center

# Nested random effects

---

# Subjects are (typically) nested within sites

```{r, fig.height=5, fig.width=5*2.2}
options(digits=5)

hippvol <- ADNIMERGE::adnimerge %>%
  select(RID, COLPROT, VISCODE, Month.bl, SITE, DX.bl, AGE, PTGENDER, ICV, 
    FLDSTRENG, FSVERSION, Hippocampus) %>%
  filter(!is.na(Hippocampus)) %>%
  arrange(RID, Month.bl) %>%
  mutate(yrs = Month.bl/12) %>%
  group_by(RID) %>%
  fill(ICV, .direction = "downup") %>%
  mutate(ICV.bl = first(ICV))

ggplot(hippvol, aes(x=yrs, y=Hippocampus, color=SITE, group=RID)) +
  geom_line(alpha=0.5) +
  facet_wrap(.~DX.bl, scales = 'free_x') +
  scale_colour_hue()
```

---

# Testing for nested random effects

```{r, echo=TRUE}
hipp_fit <- lme(Hippocampus ~ yrs*DX.bl + AGE + PTGENDER + ICV.bl,
  hippvol, random = ~yrs|RID)

hipp_fit_site <- lme(Hippocampus ~ yrs*DX.bl + AGE + PTGENDER + ICV.bl,
  hippvol, random = list(SITE = ~ yrs, RID = ~ yrs))

anova(hipp_fit_site, hipp_fit)
```

---

# Nested random effects

```{r size='tiny'}
summary(hipp_fit_site)
```

---

# Summary

.large[

Longitudinal Data Analysis methods:

- Mean structures
- Variance-covariance structures
- Mixed Model for Repeated Measures (MMRM)
- Constrained Longitudinal Analysis (cLDA)
- Nested random effects

]

---

# References (1/2)

```{r refs, echo=FALSE, results="asis", purl=FALSE}
BibOptions(check.entries = FALSE,
  bib.style = "authoryear",
  cite.style = "authoryear",
  style = "markdown",
  hyperlink = FALSE,
  dashed = FALSE,
  max.names = 10)
PrintBibliography(bib, start=1, end=8)
```

---

# References (2/2)

```{r refs2, echo=FALSE, results="asis", purl=FALSE}
PrintBibliography(bib, start=9)
```

---

class: inverse, middle, center

# Appendix

---

class: inverse, middle, center

# Missingness

---

# Missing Data: Notation

* Measurement: $Y_{ij}$ (e.g. $\textrm{ADAS}_{ij}$)
* Covariates: $X_{i}$ (e.g. treatment, gender, age, \ldots)
* Missingness indicator:
    
$$R_{ij} = \left\{
     \begin{array}{l}
       1 \textrm{ if }Y_{ij}\textrm{ is observed}\\
       0 \textrm{ otherwise}
      \end{array} \right.$$
      
* Let $\mathbf{Y}_i = (Y_{i1},\ldots,Y_{in_i})' = (\mathbf{Y}_i^o, \mathbf{Y}_i^m)$, where

$$\mathbf{Y}_i^o \textrm{ observed }Y_{ij}$$
$$\mathbf{Y}_i^m \textrm{ un-observed }Y_{ij}$$

* $D_i$ is time of dropout
* $\theta$ parameters that control $\mathbf{Y}_i$ (e.g. effects for treatment, gender, age, $\ldots$)
* $\psi$ parameters that control $\mathbf{R}_i$ (e.g. treatment effect, $\mathbf{Y}_i^m$, $\ldots$)

The notation is supposed to be a helpful shorthand. If it's not helpful, don't worry about it!

---

# Taxonomy of missing data mechanism

Missing data mechanisms are defined in terms of the assumed distribution function, $F$, or "data generating mechanism," for the missingness indicator $R_i$


| Term | Notation | Missingness depends on: |
|------|----------|-------------------------|
| Missing **Completely** at Random | $\mathbf{R}_i \sim F(X_i, \psi)$ | covariates |
| Missing at Random | $\mathbf{R}_i \sim F(X_i, \mathbf{Y}_i^o, \psi)$ | covariates & observed assessments|
| Missing **Not** at Random | $\mathbf{R}_i \sim F(X_i, \mathbf{Y}_i^o, \mathbf{Y}_i^m, \psi)$ | covariates, observed assessments, **unobserved** assessments |

We'll unpack these a bit $\ldots$

---

# Missing _Completely_ at Random (MCAR)

.large[

$\mathbf{R}_i \sim F(X_i, \psi)$: Missingness (may) depend only on observed covariates ( $X_i$)

Appropriate methods:

* Complete Case (e.g. ANCOVA) (?)
* Last Observation Carried Forward (LOCF) (?)
* Single imputation (?)

]

---

# Missing at Random (MAR)

.large[

$\mathbf{R}_i \sim F(X_i, \mathbf{Y}_i^o, \psi)$: Missingness (may) depend on observed covariates ( $X_i$) and/or observed outcomes ( $\mathbf{Y}_i^o$)

Appropriate methods:

* **Direct likelihood (e.g. mixed-effects models, MMRM, cLDA)!**,
* Multiple imputation
* Weighted generalized estimating equations (WGEE)

]

---

# Missing _Not_ at Random (MNAR)

.large[

$\mathbf{R}_i \sim F(X_i, \mathbf{Y}_i^o, \mathbf{Y}_i^m, \psi)$: Missingness (may) depend on observed covariates ( $X_i$), observed outcomes ( $\mathbf{Y}_i^o$), and unobserved outcomes ( $\mathbf{Y}_i^m$)

Appropriate sensitivity analyses (?):

* selection models: $f(\mathbf{Y}_i|X_i, \theta)f(\mathbf{R}_i|X_i, \mathbf{Y}_i^o, \mathbf{Y}_i^m, \psi)$
* pattern-mixture models (e.g. "tipping point" stress test): $f(\mathbf{Y}_i|X_i, \mathbf{R}_i, \theta)f(\mathbf{R}_i|X_i, \psi)$
* shared-parameter models: $f(\mathbf{Y}_i|X_i, \mathbf{b}_i, \theta)f(\mathbf{R}_i|X_i, \mathbf{b}_i, \psi)$

All of these approaches make untestable assumptions about missingness.

We can never completely rule out MNAR, since, if it exists, it depends on variables that we do _not_ observe.

]

---

# Missing data: bottom line

* _Missing Not at Random_ is impossible to rule out. The best we can do is _sensitivity analyses_ or apply models that make strong untestable assumptions about missingness mechanism.
* _Missing Completely At Random_ is an unrealistic and unnecessary assumption.
* _Missing at Random_ is a more reasonable assumption, and we have reliable methods that are robust in this case.

Direct likelihood (mixed-effects) is recommended. Complete case (ANCOVA), LOCF, and single imputation should be _avoided_.

---

class: inverse, middle, center

# Multiple Imputation

---

# Multiple imputation (MI)

.large[

Basic steps:

* Create multiple complete versions of the data with imputed plausible values
* Analyze each complete version with standard methods (e.g. ANCOVA)
* Combine the results

Comments:

* MI requires many more modeling decisions than direct likelihood methods (e.g. number of imputations, imputation methods, $\ldots$)
* CAUTION: Lots of nuance and complexity
* Usually reserved for _sensitivity analyses_

]

```{r results = 'hide', echo = FALSE}
# get default predictor matrix
ini_mi <- mice(trial_wide, maxit = 0, print = FALSE)
predictorMatrix <- ini_mi$predictorMatrix
# Don't want ADAS11.m12 predict by ADAS11.m18, etc.:
predictorMatrix['ADAS11.m12', 'ADAS11.m18'] <- 0
predictorMatrix['ADAS11.m6', 'ADAS11.m12'] <- 0
predictorMatrix['ADAS11.m6', 'ADAS11.m18'] <- 0
```

---

# 1. Create multiple complete versions

```{r results = 'hide', echo = TRUE}
trial_imp <- mice(trial_wide, predictorMatrix=predictorMatrix, seed = 20170714, maxit=100)
```

```{r echo = TRUE, size = 'scriptsize'}
print(head(trial_wide), digits = 2) # raw data with missing values:
print(head(complete(trial_imp)), digits = 2) # first complete version:
```

---

# 2. Analyze each complete version

```{r echo = FALSE, size = 'scriptsize'}
fits_mi <- with(data=trial_imp, lm(ADAS11.m18~active*center(ADAS11.m0)))
summary(fits_mi)
```

---

# 3. Combine the results

```{r echo = FALSE}
summary(pool(fits_mi)) %>%
  remove_rownames() %>%
  column_to_rownames(var="term") %>%
  printCoefmat()
```

---

# "Tipping point" MNAR stress test

```{r }
post <- trial_imp$post
k_tipping <- seq(0.5, 2, 0.25)
est_tipping <- lapply(k_tipping, function(tip){
  # increase imputed ADAS11.m18 in the active group by 
  # factor of k x MAR treatment estimate (3.1)
  # (nullify the imputed treatment effect to varying degrees)
  post["ADAS11.m18"] <- paste0("
  imp[[j]][data$active[!r[, j]] == 1, i] <- imp[[j]][data$active[!r[, j]] == 1, i] + ", 
  tip, 
  " * 4
  imp[[j]][data$active[!r[, j]] == 0, i] <- imp[[j]][data$active[!r[, j]] == 0, i]
  ")
  imp_k <- mice(trial_wide, post=post, predictorMatrix=predictorMatrix, seed = 20170714, maxit=100, print = FALSE)
  fit_k <- with(imp_k, lm(ADAS11.m18~active*center(ADAS11.m0)))
  bind_cols(tipping_factor = tip, summary(pool(fit_k))) %>%
    filter(term == 'active') %>%
    select(-term)
})

# inspect 4th case
tip_4 <- k_tipping[4]
post["ADAS11.m18"] <- paste0("
  imp[[j]][data$active[!r[, j]] == 1, i] <- imp[[j]][data$active[!r[, j]] == 1, i] + ", 
  tip_4, " * 4
  imp[[j]][data$active[!r[, j]] == 0, i] <- imp[[j]][data$active[!r[, j]] == 0, i]
")
imp_4 <- mice(trial_wide, post=post, predictorMatrix=predictorMatrix, seed = 20170714, maxit=100, print = FALSE)
fit_4 <- with(imp_4, lm(ADAS11.m18~active*center(ADAS11.m0)))
```

Treatment effects for different MNAR factors:

```{r echo=FALSE}
bind_rows(est_tipping) %>%
  printCoefmat(digit=4, eps=0.001)
```

---

# "Tipping point" MNAR stress test

```{r }
imputed_ids <- subset(trial_wide, is.na(ADAS11.m18))[1:5,]$id
# filter(trial_wide, id %in% imputed_ids)
```

Imputed ADAS.m18 under MAR 

```{r }
# Imputation assuming under MAR 
filter(complete(trial_imp), id %in% imputed_ids) %>%
  select(id, female, age, group, ADAS11.m0, ADAS11.m6, ADAS11.m12, ADAS11.m18)
```

Imputed ADAS.m18 under MNAR ( $k$=`r k_tipping[4]`,  $k\times 4=$ `r k_tipping[4] * 4.0` ADAS points)

```{r }
# Imputation under MNAR (k=0.5)
filter(mice::complete(imp_4), id %in% imputed_ids) %>%
  select(id, female, age, group, ADAS11.m0, ADAS11.m6, ADAS11.m12, ADAS11.m18)
```

Imputed ADAS11.m18 values are only changed for those in active group
